{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3e7dbe",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29b1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae159e7",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c123cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_FILE = 'SpamData/02_Training/train-data.txt'\n",
    "TEST_DATA_FILE = 'SpamData/02_Training/test-data.txt'\n",
    "\n",
    "TOKEN_SPAM_PROB_FILE = 'SpamData/03_Testing/prob-spam.txt'\n",
    "TOKEN_HAM_PROB_FILE = 'SpamData/03_Testing/prob-nonspam.txt'\n",
    "TOKEN_ALL_PROB_FILE = 'SpamData/03_Testing/prob-all-tokens.txt'\n",
    "\n",
    "TEST_FEATURES_MATRIX = 'SpamData/03_Testing/test-features.txt'\n",
    "TEST_TARGET_FILE = 'SpamData/03_Testing/test-target.txt'\n",
    "\n",
    "VOCAB_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5160591",
   "metadata": {},
   "source": [
    "# Read and Load Features from .txt Files into Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bf68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_data = np.loadtxt(TRAINING_DATA_FILE, delimiter=' ', dtype=int)\n",
    "sparse_test_data = np.loadtxt(TEST_DATA_FILE, delimiter=' ', dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd04de",
   "metadata": {},
   "source": [
    "# Create a Full Matrix from a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1064f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_matrix(sparse_matrix, nr_words, doc_idx=0, word_idx=1, cat_idx=2, freq_idx=3):\n",
    "    \"\"\"\n",
    "    Form a full matrix from a sparse matrix. Return a pandas dataframe.\n",
    "    Keyword Arguments:\n",
    "    sparse_matrix -- numpy array\n",
    "    nr_words -- size of the vocabulary. Total number of tokens\n",
    "    doc_idx -- position of the document id in the sparse matrix. Default: 1st column\n",
    "    word_idx -- position of the word id in the sparse matrix. Default: 2nd column\n",
    "    cat_idx -- position of the label (spam is 1, nonspam is 0). Default: 3rd column\n",
    "    freq_idx -- position of the occurence of word in sparse matrix. Default: 4th column\n",
    "    \"\"\"\n",
    "    column_names = ['DOC_ID'] + ['CATEGORY'] + list(range(0,VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:, 0])\n",
    "    full_matrix = pd.DataFrame(columns=column_names, index=doc_id_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "    \n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        label = sparse_matrix[i][cat_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, 'DOC_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, 'CATEGORY'] = label\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "        \n",
    "    full_matrix.set_index('DOC_ID', inplace=True)  \n",
    "    return full_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_train_data = make_full_matrix(sparse_train_data, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f8d01",
   "metadata": {},
   "source": [
    "# Probability of (Spam / Total Emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32695523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam is:  0.31056829511465606\n"
     ]
    }
   ],
   "source": [
    "nr_of_spam = len(full_train_data[full_train_data['CATEGORY'] == 1])\n",
    "prob_spam = nr_of_spam/full_train_data.shape[0]\n",
    "print('Probability of spam is: ',prob_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f80d47",
   "metadata": {},
   "source": [
    "# Total Number of Words (Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58aae9",
   "metadata": {},
   "source": [
    "### Counting Total number of Words (Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72270c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447611"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting all columns, except one\n",
    "full_train_features = full_train_data.loc[:, full_train_data.columns != 'CATEGORY']\n",
    "\n",
    "# Sum all values in DataFrame\n",
    "total_words = full_train_features.values.sum()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c782f3",
   "metadata": {},
   "source": [
    "# Total number of Words in Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "635d90cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193988"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_spam_emails = full_train_data[full_train_data['CATEGORY'] == 1]\n",
    "\n",
    "# Sum all NUMERIC values in DataFrame\n",
    "# If data-frame contains non-numeric data, and want to just get sum of integers, do:\n",
    "toal_words_spam = full_spam_emails.sum(numeric_only=True).sum()\n",
    "toal_words_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1c4d4",
   "metadata": {},
   "source": [
    "# Total number of Words in Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca6c7a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666147614781585"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toal_words_ham = total_words - toal_words_spam\n",
    "toal_words_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e3173",
   "metadata": {},
   "source": [
    "# Sum the tokens accross Spam and Ham Emails "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02b3ca",
   "metadata": {},
   "source": [
    "## Sum the total of each token in Spam Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2801a9bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY    1247\n",
       "0           2213\n",
       "1           1010\n",
       "2           1366\n",
       "3           2049\n",
       "            ... \n",
       "2495          12\n",
       "2496           1\n",
       "2497          26\n",
       "2498           1\n",
       "2499           3\n",
       "Length: 2501, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_spam_emails = full_train_data[full_train_data['CATEGORY'] == 1]\n",
    "\n",
    "# We have to add +1 to avoid division of 0\n",
    "summed_spam_tokens = full_spam_emails.sum(axis=0)+1\n",
    "summed_spam_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591ed4e",
   "metadata": {},
   "source": [
    "## Sum the total of each token in Ham Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e07ba008",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY       1\n",
       "0           5407\n",
       "1           2673\n",
       "2           2067\n",
       "3            909\n",
       "            ... \n",
       "2495          18\n",
       "2496          28\n",
       "2497          11\n",
       "2498           4\n",
       "2499          40\n",
       "Length: 2501, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ham_emails = full_train_data[full_train_data['CATEGORY'] == 0]\n",
    "\n",
    "# We have to add +1 to avoid division of 0\n",
    "summed_ham_tokens = full_ham_emails.sum() + 1\n",
    "summed_ham_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40076e",
   "metadata": {},
   "source": [
    "# P(Token | Spam) - Probability that a Token Occurs given the Email is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfbb0237",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY    0.006346\n",
       "0           0.011263\n",
       "1           0.005140\n",
       "2           0.006952\n",
       "3           0.010428\n",
       "              ...   \n",
       "2495        0.000061\n",
       "2496        0.000005\n",
       "2497        0.000132\n",
       "2498        0.000005\n",
       "2499        0.000015\n",
       "Length: 2501, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuz we add +1 in summed_spam_tokens, hence we are getting 1 more word for each token\n",
    "# => we're having 2500 more words in the numerator \n",
    "# => so we must add +2500 words to the denominator\n",
    "prob_tokens_spam = summed_spam_tokens / (toal_words_spam + VOCAB_SIZE) \n",
    "prob_tokens_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa7368",
   "metadata": {},
   "source": [
    "# P(Token | Ham) - Probability that a Token Occurs given the Email is NonSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "609113c3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY    0.000004\n",
       "0           0.021111\n",
       "1           0.010436\n",
       "2           0.008070\n",
       "3           0.003549\n",
       "              ...   \n",
       "2495        0.000070\n",
       "2496        0.000109\n",
       "2497        0.000043\n",
       "2498        0.000016\n",
       "2499        0.000156\n",
       "Length: 2501, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuz we add +1 in summed_spam_tokens, hence we are getting 1 more word for each token\n",
    "# => we're having 2500 more words in the numerator \n",
    "# => so we must add +2500 words to the denominator\n",
    "prob_tokens_nonspam = summed_ham_tokens / (toal_words_ham + VOCAB_SIZE) \n",
    "prob_tokens_nonspam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428898f",
   "metadata": {},
   "source": [
    "# P(Token | Ham) - Probability that a Token Occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "297dcd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.017019\n",
       "1       0.008224\n",
       "2       0.007665\n",
       "3       0.006604\n",
       "4       0.006792\n",
       "          ...   \n",
       "2495    0.000063\n",
       "2496    0.000060\n",
       "2497    0.000078\n",
       "2498    0.000007\n",
       "2499    0.000092\n",
       "Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_tokens_all = full_train_features.sum(axis=0)/total_words\n",
    "prob_tokens_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a97f5f",
   "metadata": {},
   "source": [
    "# Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90a31766",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TOKEN_HAM_PROB_FILE, prob_tokens_nonspam)\n",
    "np.savetxt(TOKEN_SPAM_PROB_FILE, prob_tokens_spam)\n",
    "np.savetxt(TOKEN_ALL_PROB_FILE, prob_tokens_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba325097",
   "metadata": {},
   "source": [
    "# Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "600a2964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "full_test_data = make_full_matrix(sparse_test_data, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b7c76c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2490</th>\n",
       "      <th>2491</th>\n",
       "      <th>2492</th>\n",
       "      <th>2493</th>\n",
       "      <th>2494</th>\n",
       "      <th>2495</th>\n",
       "      <th>2496</th>\n",
       "      <th>2497</th>\n",
       "      <th>2498</th>\n",
       "      <th>2499</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOC_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5793</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5794</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows × 2501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CATEGORY  0  1  2  3  4  5  6  7  8  ...  2490  2491  2492  2493  \\\n",
       "DOC_ID                                       ...                           \n",
       "8              1  0  0  1  4  2  1  2  4  1  ...     0     0     0     0   \n",
       "12             1  6  1  1  0  1  0  0  1  0  ...     0     0     0     0   \n",
       "14             1  0  0  1  1  0  0  0  0  0  ...     0     0     0     0   \n",
       "15             1  0  2  1  1  2  0  0  3  0  ...     0     0     0     0   \n",
       "17             1  0  0  0  0  1  0  1  0  0  ...     0     0     0     0   \n",
       "...          ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   \n",
       "5789           0  5  5  2  2  1  2  0  0  1  ...     0     0     0     0   \n",
       "5790           0  2  0  1  1  0  1  0  0  0  ...     0     0     0     0   \n",
       "5791           0  0  4  0  2  4  3  3  1  4  ...     0     0     0     0   \n",
       "5793           0  1  1  1  0  0  0  1  0  0  ...     0     0     0     0   \n",
       "5794           0  3  1  0  1  1  0  0  0  1  ...     0     0     0     0   \n",
       "\n",
       "        2494  2495  2496  2497  2498  2499  \n",
       "DOC_ID                                      \n",
       "8          0     0     0     0     0     0  \n",
       "12         0     0     0     0     0     0  \n",
       "14         0     0     0     0     0     0  \n",
       "15         0     0     0     0     0     0  \n",
       "17         0     0     0     0     0     0  \n",
       "...      ...   ...   ...   ...   ...   ...  \n",
       "5789       0     0     0     0     0     0  \n",
       "5790       0     0     0     0     0     0  \n",
       "5791       0     2     0     0     0     0  \n",
       "5793       0     0     0     0     0     0  \n",
       "5794       0     0     0     0     0     0  \n",
       "\n",
       "[1725 rows x 2501 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09f67066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = full_test_data.loc[:, full_test_data.columns != 'CATEGORY']\n",
    "y_test = full_test_data.CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06a7c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TEST_TARGET_FILE, y_test)\n",
    "np.savetxt(TEST_FEATURES_MATRIX, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba531b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
